{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "214a8215-9e6e-41c1-b37e-4c58aff5eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ[\"TENSORBOARD_BINARY\"] = \".speech_recognition/bin/tensorboard\"\n",
    "\n",
    "# tensorboard --logdir ./tensorboard --host \"0.0.0.0\" --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5465ed92-c8f5-408d-ad77-63f5d626ef37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DTypePolicy \"mixed_float16\">"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.optimizers import LossScaleOptimizer\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "os.makedirs(\"training_images\", exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "log_dir = \"tensorboard/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "char_to_num = {char: idx + 1 for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \")}\n",
    "char_to_num['<PAD>'] = 0\n",
    "num_to_char = {v: k for k, v in char_to_num.items()}\n",
    "n_mfcc = 13\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "mixed_precision.global_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5677f823-21b4-4a67-acc1-738cc6c52dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_size(dataset_filename: str):\n",
    "    \"\"\"Counts the number of samples in the HDF5 dataset\"\"\"\n",
    "    with h5py.File(f\"data/CommonVoice/{dataset_filename}\", \"r\") as hf:\n",
    "        mfcc_group = hf[\"mfcc\"]\n",
    "        labels_group = hf[\"labels\"]\n",
    "        i = 0\n",
    "        for key in mfcc_group.keys():\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def dataset_from_generator(h5_file_path):\n",
    "    def generator():\n",
    "        with h5py.File(h5_file_path, \"r\") as hf:\n",
    "            mfcc_group = hf[\"mfcc\"]\n",
    "            labels_group = hf[\"labels\"]\n",
    "            for key in mfcc_group.keys():\n",
    "                yield mfcc_group[key][:], labels_group[key][:]\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 13), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        ))\n",
    "\n",
    "def generate_padded_data(h5_file_path, batch_size=BATCH_SIZE):\n",
    "    dataset = dataset_from_generator(h5_file_path)\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=batch_size,\n",
    "        padded_shapes=([None, 13], [None]),\n",
    "        padding_values=(0.0, char_to_num['<PAD>'])\n",
    "    )\n",
    "    return dataset.cache()\n",
    "\n",
    "\n",
    "def plot_loss_curves(history, model_name):\n",
    "    \"\"\"Return separate loss curves for training and validation results.\"\"\"\n",
    "    \n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(epochs, loss, label=\"training loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"val loss\")\n",
    "    plt.title(f\"CTC loss for {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"training_images/{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def build_model(model_name, layer_list, input_shape=(None, 13), vocab_size=len(char_to_num), learning_rate=1e-03, load_weights=\"\"):\n",
    "    \"\"\"\n",
    "    Creates a model from a list of layers using keras functional api; input and output is permanently defined.\n",
    "    The returned model is compiled using CTC and Loss scale optimizer.\n",
    "    \"\"\"\n",
    "        \n",
    "    inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "    x = inputs\n",
    "    for layer in layer_list:\n",
    "        x = layer(x)\n",
    "\n",
    "    outputs = layers.Dense(units=vocab_size, name=\"output_layer\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    if len(load_weights) > 0:\n",
    "         model.load_weights(f\"checkpoints/{load_weights}.ckpt.weights.h5\", skip_mismatch=True)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=learning_rate)),\n",
    "        loss=tf.keras.losses.CTC())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                train_data,\n",
    "                val_data,\n",
    "                train_data_size,\n",
    "                val_data_size,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=20,\n",
    "                initial_epoch=0,\n",
    "                callbacks=None):\n",
    "    \"Trains model, plots and returns history.\"\n",
    "    \n",
    "    history = model.fit(\n",
    "                train_data,\n",
    "                validation_data=val_data,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=int(train_data_size/batch_size),\n",
    "                validation_steps=int(val_data_size/batch_size),\n",
    "                callbacks=callbacks,\n",
    "                verbose=1,\n",
    "                initial_epoch=initial_epoch\n",
    "                )\n",
    "    \n",
    "    plot_loss_curves(history, model.name)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cec0a7-5266-46bd-bc64-991d8de7e961",
   "metadata": {},
   "source": [
    "## CTC - Connectionist Temporal Classification\n",
    "* Enables comparative analysis of sequences with different lengths, where there is no clear alignment between input and output, e.g., audio and transcription.\n",
    "* Predictions are in the form of logits, blank index - 0; no additional function is used in the output layer.\n",
    "* Loss value is normalized by default at the batch level - allows comparison of batches with samples of varying sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb691d9f-fa93-40c2-80ce-390128bc20a7",
   "metadata": {},
   "source": [
    "# USA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa5db55-6047-4a79-8f15-54c53e29f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 90322, Val: 10036, Test: 25090\n"
     ]
    }
   ],
   "source": [
    "train_data_size = check_dataset_size(\"train.h5\")\n",
    "train_dataset = (generate_padded_data(\"data/CommonVoice/train.h5\", batch_size=BATCH_SIZE)\n",
    "                 .shuffle(buffer_size=train_data_size//BATCH_SIZE, reshuffle_each_iteration=True)\n",
    "                 .repeat()\n",
    "                 .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_data_size = check_dataset_size(\"val.h5\")\n",
    "val_dataset = generate_padded_data(\"data/CommonVoice/val.h5\", batch_size=BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "test_data_size = check_dataset_size(\"test.h5\")\n",
    "test_dataset = generate_padded_data(\"data/CommonVoice/test.h5\", batch_size=BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Train: {int(train_data_size)}, Val: {int(val_data_size)}, Test: {test_data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad767a2a-9f97-485d-b47a-eb4ff3877851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_06a2b_row0_col1, #T_06a2b_row0_col2, #T_06a2b_row0_col3 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_06a2b_row1_col1 {\n",
       "  background-color: #9cb9d9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row1_col2, #T_06a2b_row1_col3 {\n",
       "  background-color: #8fb4d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row2_col1, #T_06a2b_row5_col2, #T_06a2b_row5_col3 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row2_col2 {\n",
       "  background-color: #c4cbe3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row2_col3 {\n",
       "  background-color: #d2d2e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row3_col1 {\n",
       "  background-color: #ebe6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row3_col2 {\n",
       "  background-color: #a8bedc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row3_col3 {\n",
       "  background-color: #afc1dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row4_col1 {\n",
       "  background-color: #f4edf6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row4_col2 {\n",
       "  background-color: #fbf4f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row4_col3 {\n",
       "  background-color: #fdf5fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_06a2b_row5_col1 {\n",
       "  background-color: #e5e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_06a2b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_06a2b_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_06a2b_level0_col1\" class=\"col_heading level0 col1\" >Train loss</th>\n",
       "      <th id=\"T_06a2b_level0_col2\" class=\"col_heading level0 col2\" >Val loss</th>\n",
       "      <th id=\"T_06a2b_level0_col3\" class=\"col_heading level0 col3\" >Test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_06a2b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_06a2b_row0_col0\" class=\"data row0 col0\" >BLSTM64x3_USA_b128</td>\n",
       "      <td id=\"T_06a2b_row0_col1\" class=\"data row0 col1\" >67.938469</td>\n",
       "      <td id=\"T_06a2b_row0_col2\" class=\"data row0 col2\" >68.590004</td>\n",
       "      <td id=\"T_06a2b_row0_col3\" class=\"data row0 col3\" >69.356064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06a2b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_06a2b_row1_col0\" class=\"data row1 col0\" >BLSTM128x3_USA_b128</td>\n",
       "      <td id=\"T_06a2b_row1_col1\" class=\"data row1 col1\" >43.971088</td>\n",
       "      <td id=\"T_06a2b_row1_col2\" class=\"data row1 col2\" >52.030102</td>\n",
       "      <td id=\"T_06a2b_row1_col3\" class=\"data row1 col3\" >52.767967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06a2b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_06a2b_row2_col0\" class=\"data row2 col0\" >BLSTM256x3_USA_b128</td>\n",
       "      <td id=\"T_06a2b_row2_col1\" class=\"data row2 col1\" >27.838634</td>\n",
       "      <td id=\"T_06a2b_row2_col2\" class=\"data row2 col2\" >47.835159</td>\n",
       "      <td id=\"T_06a2b_row2_col3\" class=\"data row2 col3\" >47.276829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06a2b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_06a2b_row3_col0\" class=\"data row3 col0\" >BLSTM256x3_USA_b128_l2</td>\n",
       "      <td id=\"T_06a2b_row3_col1\" class=\"data row3 col1\" >33.086040</td>\n",
       "      <td id=\"T_06a2b_row3_col2\" class=\"data row3 col2\" >50.292000</td>\n",
       "      <td id=\"T_06a2b_row3_col3\" class=\"data row3 col3\" >50.354992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06a2b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_06a2b_row4_col0\" class=\"data row4 col0\" >BLSTM256x3_USA_b128_dropout</td>\n",
       "      <td id=\"T_06a2b_row4_col1\" class=\"data row4 col1\" >30.834980</td>\n",
       "      <td id=\"T_06a2b_row4_col2\" class=\"data row4 col2\" >40.273403</td>\n",
       "      <td id=\"T_06a2b_row4_col3\" class=\"data row4 col3\" >40.723652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06a2b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_06a2b_row5_col0\" class=\"data row5 col0\" >BLSTM256x3_USA_b128_spdropout</td>\n",
       "      <td id=\"T_06a2b_row5_col1\" class=\"data row5 col1\" >34.111958</td>\n",
       "      <td id=\"T_06a2b_row5_col2\" class=\"data row5 col2\" >39.546360</td>\n",
       "      <td id=\"T_06a2b_row5_col3\" class=\"data row5 col3\" >40.216934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x73778c323c20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example tests\n",
    "\n",
    "models = {\n",
    "    f\"BLSTM64x3_USA_b{BATCH_SIZE}\": [\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"blstm_3\"),\n",
    "    ],\n",
    "    f\"BLSTM128x3_USA_b{BATCH_SIZE}\": [\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"blstm_3\")\n",
    "    ],\n",
    "    f\"BLSTM256x3_USA_b{BATCH_SIZE}\": [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "    ],\n",
    "    f\"BLSTM256x3_USA_b{BATCH_SIZE}_l2\": [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.001)), name=\"blstm_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.001)), name=\"blstm_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.001)), name=\"blstm_3\"),\n",
    "    ],\n",
    "    f\"BLSTM256x3_USA_b{BATCH_SIZE}_dropout\": [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.Dropout(0.2, name=\"dropout_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.Dropout(0.2, name=\"dropout_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "        layers.Dropout(0.2, name=\"final_dropout\")\n",
    "    ],\n",
    "    f\"BLSTM256x3_USA_b{BATCH_SIZE}_spdropout\": [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "        layers.Dropout(0.2, name=\"final_dropout\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for name, layer_list in models.items():  \n",
    "    model = build_model(name, layer_list)\n",
    "    print(f\"Training model {name}....\")\n",
    "    # print(model.summary())\n",
    "\n",
    "    experiment_dir = f\"{log_dir}usa/{name}\"\n",
    "\n",
    "    callbacks = [\n",
    "    ModelCheckpoint(f\"checkpoints/{name}.ckpt.weights.h5\",\n",
    "                   save_weights_only=True),\n",
    "    EarlyStopping(patience=3,\n",
    "                  restore_best_weights=True),\n",
    "    TensorBoard(log_dir=experiment_dir,\n",
    "                histogram_freq=1,\n",
    "                write_steps_per_second=True)\n",
    "    ]\n",
    "    \n",
    "    history = train_model(model, train_dataset, val_dataset,\n",
    "                          callbacks=callbacks,\n",
    "                          train_data_size=train_data_size,\n",
    "                          val_data_size=val_data_size,\n",
    "                          epochs=20)\n",
    "\n",
    "    results = {\n",
    "        \"Model\": name,\n",
    "        \"Params\": model.count_params(),\n",
    "        \"Train loss\": history.history[\"loss\"][-1],\n",
    "        \"Val loss\": history.history[\"val_loss\"][-1],\n",
    "        \"Test loss\": model.evaluate(test_dataset, verbose=0, steps=test_data_size//BATCH_SIZE)\n",
    "    }\n",
    "    all_results.append(results)\n",
    "\n",
    "all_results = pd.DataFrame(all_results)\n",
    "all_results.to_csv(\"results_00.csv\", index=False)\n",
    "all_results.style.background_gradient()\n",
    "# all_results.iloc[:, [0, 2, 3, 4]].style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c4bd2-41d5-4768-bfac-377a63e9da36",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90de9bd2-c376-43db-b888-f240901caa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_mfcc(mfcc, label):\n",
    "\n",
    "    # gaussian noise\n",
    "    if tf.random.uniform(()) > 0.3:\n",
    "        noise_std = tf.random.uniform([], 0.005, 0.025)\n",
    "        noise = tf.random.normal(tf.shape(mfcc), stddev=noise_std)\n",
    "        mfcc = mfcc + noise\n",
    "    \n",
    "    # frequency masking\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        num_mfcc = tf.shape(mfcc)[0]  # 13\n",
    "        max_freq_mask = 2 \n",
    "        \n",
    "        freq_start = tf.random.uniform([], 0, num_mfcc - max_freq_mask, dtype=tf.int32)\n",
    "        freq_len = tf.random.uniform([], 1, max_freq_mask + 1, dtype=tf.int32)\n",
    "        \n",
    "        freq_indices = tf.range(num_mfcc)\n",
    "        freq_mask = tf.logical_and(freq_indices >= freq_start, \n",
    "                                  freq_indices < freq_start + freq_len)\n",
    "        \n",
    "        mfcc = tf.where(freq_mask[:, None], 0.0, mfcc)\n",
    "    \n",
    "    # time masking \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        seq_len = tf.shape(mfcc)[1]\n",
    "        \n",
    "        max_time_mask = tf.maximum(1, seq_len // 10)  # 10%\n",
    "        max_time_mask = tf.minimum(max_time_mask, 10)  # max 10 time steps\n",
    "        \n",
    "        safe_range = tf.maximum(1, seq_len - max_time_mask)\n",
    "        time_start = tf.random.uniform([], 0, safe_range, dtype=tf.int32)\n",
    "        time_len = tf.random.uniform([], 1, max_time_mask + 1, dtype=tf.int32)\n",
    "        \n",
    "        time_indices = tf.range(seq_len)\n",
    "        time_mask = tf.logical_and(time_indices >= time_start,\n",
    "                                  time_indices < time_start + time_len)\n",
    "\n",
    "        mfcc = tf.where(time_mask[None, :], 0.0, mfcc)\n",
    "    \n",
    "    # amplitude scaling\n",
    "    if tf.random.uniform(()) > 0.4:\n",
    "        scale = tf.random.uniform([], 0.85, 1.15)\n",
    "        mfcc = mfcc * scale\n",
    "    \n",
    "    # feature dropout\n",
    "    if tf.random.uniform(()) > 0.6:\n",
    "        dropout_prob = tf.random.uniform([], 0.05, 0.15)\n",
    "        dropout_mask = tf.random.uniform(tf.shape(mfcc)) > dropout_prob\n",
    "        mfcc = tf.where(dropout_mask, mfcc, 0.0)\n",
    "    \n",
    "    return mfcc, label\n",
    "\n",
    "\n",
    "def dataset_from_generator(h5_file_path, train=True, batch_size=BATCH_SIZE):\n",
    "    def generator():\n",
    "        with h5py.File(h5_file_path, \"r\") as hf:\n",
    "            mfcc_group = hf[\"mfcc\"]\n",
    "            labels_group = hf[\"labels\"]\n",
    "            for key in mfcc_group.keys():\n",
    "                yield mfcc_group[key][:], labels_group[key][:]\n",
    "\n",
    "    if train:\n",
    "    \n",
    "        return (tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(None, 13), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "            ))\n",
    "                .cache()\n",
    "                .repeat()\n",
    "                .map(augment_mfcc, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .padded_batch(batch_size=batch_size,\n",
    "                              padded_shapes=([None, 13], [None]),\n",
    "                              padding_values=(0.0, char_to_num['<PAD>']))\n",
    "               .shuffle(buffer_size=train_data_size//batch_size, reshuffle_each_iteration=True)\n",
    "               .prefetch(tf.data.AUTOTUNE)\n",
    "               )\n",
    "        \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 13), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        ))\n",
    "\n",
    "\n",
    "def generate_padded_data(h5_file_path, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    dataset = dataset_from_generator(h5_file_path, train=False)\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=batch_size,\n",
    "        padded_shapes=([None, 13], [None]),\n",
    "        padding_values=(0.0, char_to_num['<PAD>'])\n",
    "    )\n",
    "    return dataset.cache().repeat().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24f95c6-cab5-4bff-8460-64b6b9902be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 90322, Val: 10036, Test: 25090\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_from_generator(\"data/CommonVoice/train.h5\", train=True)\n",
    "\n",
    "test_dataset = generate_padded_data(\"data/CommonVoice/test.h5\")\n",
    "test_data_size = check_dataset_size(\"test.h5\")\n",
    "\n",
    "val_data_size = check_dataset_size(\"val.h5\")\n",
    "val_dataset = generate_padded_data(\"data/CommonVoice/val.h5\")\n",
    "\n",
    "print(f\"Train: {int(train_data_size)}, Val: {int(val_data_size)}, Test: {test_data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6e11c27-d9be-4141-9a6b-5cfd987ad086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4aca9_row0_col1, #T_4aca9_row0_col2, #T_4aca9_row0_col3 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4aca9_row1_col1, #T_4aca9_row2_col2, #T_4aca9_row2_col3 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4aca9_row1_col2, #T_4aca9_row1_col3 {\n",
       "  background-color: #f3edf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4aca9_row2_col1 {\n",
       "  background-color: #ede7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4aca9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4aca9_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_4aca9_level0_col1\" class=\"col_heading level0 col1\" >Train loss</th>\n",
       "      <th id=\"T_4aca9_level0_col2\" class=\"col_heading level0 col2\" >Val loss</th>\n",
       "      <th id=\"T_4aca9_level0_col3\" class=\"col_heading level0 col3\" >Test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4aca9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4aca9_row0_col0\" class=\"data row0 col0\" >BLSTM128x3_USA_b128_augm</td>\n",
       "      <td id=\"T_4aca9_row0_col1\" class=\"data row0 col1\" >52.819092</td>\n",
       "      <td id=\"T_4aca9_row0_col2\" class=\"data row0 col2\" >49.725716</td>\n",
       "      <td id=\"T_4aca9_row0_col3\" class=\"data row0 col3\" >50.191483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4aca9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4aca9_row1_col0\" class=\"data row1 col0\" >BLSTM256x3_USA_b128_dropout_augm</td>\n",
       "      <td id=\"T_4aca9_row1_col1\" class=\"data row1 col1\" >40.085445</td>\n",
       "      <td id=\"T_4aca9_row1_col2\" class=\"data row1 col2\" >37.723663</td>\n",
       "      <td id=\"T_4aca9_row1_col3\" class=\"data row1 col3\" >38.406605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4aca9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4aca9_row2_col0\" class=\"data row2 col0\" >BLSTM256x3_USA_b128_spdropout_augm</td>\n",
       "      <td id=\"T_4aca9_row2_col1\" class=\"data row2 col1\" >41.654919</td>\n",
       "      <td id=\"T_4aca9_row2_col2\" class=\"data row2 col2\" >36.684639</td>\n",
       "      <td id=\"T_4aca9_row2_col3\" class=\"data row2 col3\" >37.386055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x73782adb2810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    f\"BLSTM128x3_USA_b{BATCH_SIZE}_augm\": [\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"blstm_3\")\n",
    "    ],\n",
    "    f\"BLSTM256x3_USA_b{BATCH_SIZE}_dropout_augm\": [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.Dropout(0.2, name=\"dropout_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.Dropout(0.2, name=\"dropout_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "        layers.Dropout(0.2, name=\"final_dropout\")\n",
    "    ],\n",
    "    f\"BLSTM256x3_USA_b{BATCH_SIZE}_spdropout_augm\": [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "        layers.Dropout(0.2, name=\"final_dropout\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "experiment_dir = f\"{log_dir}usa/augm/{name}\"\n",
    "\n",
    "callbacks = [\n",
    "ModelCheckpoint(f\"checkpoints/{name}.ckpt.weights.h5\",\n",
    "               save_weights_only=True),\n",
    "EarlyStopping(patience=3,\n",
    "              restore_best_weights=True),\n",
    "TensorBoard(log_dir=experiment_dir,\n",
    "            histogram_freq=1,\n",
    "            write_steps_per_second=True)\n",
    "]\n",
    "\n",
    "for name, layer_list in models.items():  \n",
    "    model = build_model(name, layer_list)\n",
    "    print(f\"Training model {name}....\")\n",
    "    # print(model.summary())\n",
    "\n",
    "    history = train_model(model, train_dataset, val_dataset,\n",
    "                          callbacks=callbacks,\n",
    "                          train_data_size=train_data_size,\n",
    "                          val_data_size=val_data_size,\n",
    "                          epochs=30)\n",
    "\n",
    "    results = {\n",
    "        \"Model\": name,\n",
    "        \"Train loss\": history.history[\"loss\"][-1],\n",
    "        \"Val loss\": history.history[\"val_loss\"][-1],\n",
    "        \"Test loss\": model.evaluate(test_dataset, verbose=0, steps=test_data_size//BATCH_SIZE)\n",
    "    }\n",
    "    all_results.append(results)\n",
    "\n",
    "all_results = pd.DataFrame(all_results)\n",
    "all_results.to_csv(\"results_augm.csv\", index=False)\n",
    "\n",
    "all_results.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0f5d828-9909-4cef-8345-34ffa7ce6a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLSTM256x3_USA_b128_spdropout_augm</td>\n",
       "      <td>3717148</td>\n",
       "      <td>32.96</td>\n",
       "      <td>33.08</td>\n",
       "      <td>33.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model   Params  Train loss  Val loss  \\\n",
       "0  BLSTM256x3_USA_b128_spdropout_augm  3717148       32.96     33.08   \n",
       "\n",
       "   Test loss  \n",
       "0      33.78  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = f\"BLSTM256x3_USA_b{BATCH_SIZE}_spdropout_augm\"\n",
    "\n",
    "# model = build_model(name, layer_list, load_weights=name)\n",
    "\n",
    "history = train_model(model, train_dataset, val_dataset,\n",
    "                      callbacks=callbacks,\n",
    "                      train_data_size=train_data_size,\n",
    "                      val_data_size=val_data_size,\n",
    "                      epochs=60,\n",
    "                      initial_epoch=30)\n",
    "\n",
    "results = {\n",
    "    \"Model\": name,\n",
    "    \"Params\": model.count_params(),\n",
    "    \"Train loss\": history.history[\"loss\"][-1],\n",
    "    \"Val loss\": history.history[\"val_loss\"][-1],\n",
    "    \"Test loss\": model.evaluate(test_dataset, verbose=0, steps=test_data_size//BATCH_SIZE)\n",
    "}\n",
    "pd.DataFrame(results, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e57d41e3-5f30-4108-8043-1c934424308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"usa_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7aac67-69a8-4377-93c2-28ad0fe1942c",
   "metadata": {},
   "source": [
    "# Different accents, balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb55b555-1789-4f56-8461-1770a1ca5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 79236, Val: 8805, Test: 22011\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_from_generator(\"data/CommonVoice/balanced_train.h5\", train=True)\n",
    "train_data_size = check_dataset_size(\"balanced_train.h5\")\n",
    "\n",
    "test_dataset = generate_padded_data(\"data/CommonVoice/balanced_test.h5\")\n",
    "test_data_size = check_dataset_size(\"balanced_test.h5\")\n",
    "\n",
    "val_data_size = check_dataset_size(\"balanced_val.h5\")\n",
    "val_dataset = generate_padded_data(\"data/CommonVoice/balanced_val.h5\")\n",
    "\n",
    "print(f\"Train: {int(train_data_size)}, Val: {int(val_data_size)}, Test: {test_data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1781b83-7895-45d0-91b5-cea0e9b9ba36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLSTM256x3_balanced_b128_spdropout_augm</td>\n",
       "      <td>3717148</td>\n",
       "      <td>45.90</td>\n",
       "      <td>45.94</td>\n",
       "      <td>46.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model   Params  Train loss  Val loss  \\\n",
       "0  BLSTM256x3_balanced_b128_spdropout_augm  3717148       45.90     45.94   \n",
       "\n",
       "   Test loss  \n",
       "0      46.21  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = f\"BLSTM256x3_balanced_b{BATCH_SIZE}_spdropout_augm\"\n",
    "layer_list = [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "        layers.Dropout(0.2, name=\"final_dropout\")\n",
    "]\n",
    "\n",
    "model = build_model(name, layer_list)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-03)),\n",
    "    loss=\"ctc\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "experiment_dir = f\"{log_dir}{name}\"\n",
    "callbacks = [\n",
    "ModelCheckpoint(\"checkpoints/{model_name}_epoch{{epoch:02d}}.ckpt.weights.h5\".format(model_name=name),\n",
    "               save_weights_only=True, save_freq=\"epoch\"),\n",
    "EarlyStopping(patience=4,\n",
    "              restore_best_weights=True),\n",
    "TensorBoard(log_dir=experiment_dir,\n",
    "            histogram_freq=1,\n",
    "            write_steps_per_second=True),\n",
    "ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=2, min_lr=1e-07)\n",
    "]\n",
    "\n",
    "history = train_model(model, train_dataset, val_dataset,\n",
    "                      callbacks=callbacks,\n",
    "                      train_data_size=train_data_size,\n",
    "                      val_data_size=val_data_size,\n",
    "                      epochs=50,\n",
    "                      initial_epoch=0)\n",
    "\n",
    "results = {\n",
    "    \"Model\": name,\n",
    "    \"Params\": model.count_params(),\n",
    "    \"Train loss\": history.history[\"loss\"][-1],\n",
    "    \"Val loss\": history.history[\"val_loss\"][-1],\n",
    "    \"Test loss\": model.evaluate(test_dataset, verbose=0, steps=test_data_size//BATCH_SIZE)\n",
    "}\n",
    "pd.DataFrame(results, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10cb43e2-9587-42b7-bc9f-f8f7352a068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mixed_acc_balanced.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54859c-fb76-4f8d-9f10-9b30404c15b9",
   "metadata": {},
   "source": [
    "# Different acccents, unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc035e5f-91d0-449b-af41-9f2ecc6f6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 76709, Val: 8524, Test: 21309\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data_size = check_dataset_size(\"unbalanced_train.h5\")\n",
    "train_dataset = dataset_from_generator(\"data/CommonVoice/unbalanced_train.h5\", train=True, train_data_size=train_data_size)\n",
    "\n",
    "test_dataset = generate_padded_data(\"data/CommonVoice/unbalanced_test.h5\")\n",
    "test_data_size = check_dataset_size(\"unbalanced_test.h5\")\n",
    "\n",
    "val_data_size = check_dataset_size(\"unbalanced_val.h5\")\n",
    "val_dataset = generate_padded_data(\"data/CommonVoice/unbalanced_val.h5\")\n",
    "\n",
    "print(f\"Train: {int(train_data_size)}, Val: {int(val_data_size)}, Test: {test_data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ccff2db-1044-4a46-8fa0-01c5b4dbcf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 47.844\n"
     ]
    }
   ],
   "source": [
    "name = f\"BLSTM256x3_unbalanced_b{BATCH_SIZE}_spdropout_augm\"\n",
    "layer_list = [\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_1\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_1\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_2\"),\n",
    "        layers.SpatialDropout1D(0.2, name=\"spatial_dropout_2\"),\n",
    "        layers.Bidirectional(layers.LSTM(256, return_sequences=True), name=\"blstm_3\"),\n",
    "        layers.Dropout(0.2, name=\"final_dropout\")\n",
    "]\n",
    "\n",
    "model = build_model(name, layer_list)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-03)),\n",
    "    loss=\"ctc\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "experiment_dir = f\"{log_dir}{name}\"\n",
    "callbacks = [\n",
    "ModelCheckpoint(\"checkpoints/{model_name}_epoch{{epoch:02d}}.ckpt.weights.h5\".format(model_name=name),\n",
    "               save_weights_only=True, save_freq=\"epoch\"),\n",
    "EarlyStopping(patience=4,\n",
    "              restore_best_weights=True),\n",
    "TensorBoard(log_dir=experiment_dir,\n",
    "            histogram_freq=1,\n",
    "            write_steps_per_second=True),\n",
    "ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=2, min_lr=1e-07)\n",
    "]\n",
    "\n",
    "history = train_model(model, train_dataset, val_dataset,\n",
    "                      callbacks=callbacks,\n",
    "                      train_data_size=train_data_size,\n",
    "                      val_data_size=val_data_size,\n",
    "                      epochs=50,\n",
    "                      initial_epoch=0)\n",
    "\n",
    "print(f\"Test loss: {round(model.evaluate(test_dataset, verbose=0, steps=test_data_size//BATCH_SIZE), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "df9caf12-ff46-4160-a1b9-251908b03f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mixed_acc_unabalanced.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_recognition",
   "language": "python",
   "name": "speech_recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
